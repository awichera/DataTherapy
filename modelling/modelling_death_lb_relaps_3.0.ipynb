{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e8ff5f8-ecd9-46dc-bf9e-ef83b87324f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import mlflow\n",
    "from pyspark.sql import functions as f\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, classification_report, precision_recall_curve, average_precision_score\n",
    "import seaborn as sns\n",
    "from pyspark.sql.types import IntegerType\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c5516d8-74f4-4491-bd93-5d8ac919afd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "feature_cols_post_care = [\n",
    "#\"id\",\n",
    " #### diagnoza ####\n",
    " 'diagnoza_detail_kod', # cat IMPORTANT!\n",
    " # 'diagnoza_kod', # vzdy c50, nema smysl\n",
    " 'lateralita_kod', # num\n",
    " 'topografie_kod', # cat zahodit?\n",
    " 'morfologie_kod', # cat zahodit?\n",
    " 'grading', # numerizujeme ###\n",
    " 'tnm_klasifikace_t_kod', # numerizujeme\n",
    " 'tnm_klasifikace_n_kod', # numerizujeme\n",
    " 'tnm_klasifikace_m_kod', # numerizujeme\n",
    " 'tnm_klasifikace_metastazy_oth', # bool\n",
    " 'tnm_klasifikace_metastazy_ski', # bool\n",
    " 'tnm_klasifikace_metastazy_lym', # bool\n",
    " 'tnm_klasifikace_metastazy_adr', # bool\n",
    " 'tnm_klasifikace_metastazy_bra', # bool\n",
    " 'tnm_klasifikace_metastazy_per', # bool\n",
    " 'tnm_klasifikace_metastazy_hep', # bool\n",
    " 'tnm_klasifikace_metastazy_ple', # bool\n",
    " 'tnm_klasifikace_metastazy_oss', # bool\n",
    " 'tnm_klasifikace_metastazy_mar', # bool\n",
    " 'tnm_klasifikace_metastazy_pul', # bool\n",
    " 'tnm_klasifikace_metastazy_xxx', # bool\n",
    " # 'tnm_klasifikace_vysledna', zahazujeme duplikace \n",
    " 'stadium', # numerizujeme\n",
    " # 'rok_dg',\n",
    " # 'mesic_dg',\n",
    " \n",
    " #### pacient ####\n",
    " 'vekova_kategorie_10let_dg', # numerizujeme\n",
    " 'novotvar_poradi', # num\n",
    " 'novotvar_poradi_dg', # bool\n",
    " 'novotvar_poradi_dg_skupina', # bool\n",
    " 'novotvar_poradi_dg_maligni', # num\n",
    " 'novotvar_poradi_dg_maligni_bez_c44',\n",
    " \n",
    " #### pridruzene diagnozy ####\n",
    " 'DCCI', # num\n",
    " 'dcci_infarkt_myokardu', # bool\n",
    " 'dcci_srdecni_selhani', # bool\n",
    " 'dcci_onemocneni_perifernich_cev', # bool\n",
    " 'dcci_cevni_nemoci_mozku', # bool\n",
    " 'dcci_demence', # bool\n",
    " 'dcci_chronicke_plicni_onemocneni', # bool\n",
    " 'dcci_onemocneni_pojivovych_tkani', # bool\n",
    " 'dcci_vredova_onemocneni', # bool\n",
    " 'dcci_mirna_onemocneni_jater', # bool\n",
    " 'dcci_diabetes_bez_chronickych_komplikaci', # bool\n",
    " 'dcci_diabetes_s_chronickymi_komplikacemi', # bool\n",
    " 'dcci_hemiplegia_or_paraplegia', # bool\n",
    " 'dcci_onemocneni_ledvin', # bool\n",
    " 'dcci_nadorova_onemocneni', # bool\n",
    " 'dcci_stredne_zavazne_nebo_vazne_onemocneni_jater', # bool\n",
    " 'dcci_nadorova_onemocneni_s_metastazemi', # bool\n",
    " 'time_indexdate_to_datum_dg', # num\n",
    " \n",
    " #### vysetreni pri primarni diagnoze ####\n",
    " 'pd_ct', # bool\n",
    " 'pd_petct', # bool\n",
    " 'pd_mr', # bool\n",
    " 'pd_scint', # bool\n",
    " 'pd_spect', # bool\n",
    " 'pd_rtg', # bool\n",
    " 'pd_sono', # bool\n",
    " 'pd_mamo', # bool\n",
    " 'pd_screening', # bool\n",
    " 'pd_jina', # bool\n",
    "\n",
    " #### Provedeni konsilia ####\n",
    " 'je_mdt',\n",
    " 'time_indexdate_to_mdt',\n",
    " \n",
    " ### # Kde je pacient lecen ####\n",
    " # 'je_pl', # zahodit, nelecene vyhodime z datasetu\n",
    " 'je_pl_koc',\n",
    " 'je_pl_roc',\n",
    " 'je_pl_jinde',\n",
    "\n",
    " #### Delka a zpozdeni zahajeni prim lecby ####\n",
    " 'time_datum_dg_to_zahajeni_pl',\n",
    " 'pl_delka',\n",
    " \n",
    " #### Uzite modality lecby #### \n",
    " 'je_pl_oper',\n",
    " 'je_pl_radio',\n",
    " 'je_pl_target',\n",
    " 'je_pl_chemo',\n",
    " 'je_pl_hormo',\n",
    " 'je_pl_imuno',\n",
    " 'pl_pocet_leceb',\n",
    "\n",
    "#### Poradi modalit lecby #### \n",
    "#  'pl_typ_lecby_1',\n",
    "#  'pl_kod_lecby_1',\n",
    "#  'pl_typ_lecby_2',\n",
    "#  'pl_kod_lecby_2',\n",
    "#  'pl_typ_lecby_3',\n",
    "#  'pl_kod_lecby_3',\n",
    "#  'pl_typ_lecby_4',\n",
    "#  'pl_kod_lecby_4',\n",
    "#  'pl_typ_lecby_5',\n",
    "#  'pl_kod_lecby_5',\n",
    "#  'pl_typ_lecby_6',\n",
    "#  'pl_kod_lecby_6',\n",
    "#  'pl_typ_lecby_7',\n",
    "#  'pl_kod_lecby_7',\n",
    "#  'pl_typ_lecby_8',\n",
    "#  'pl_kod_lecby_8',\n",
    "#  'pl_typ_lecby_9',\n",
    "#  'pl_kod_lecby_9',\n",
    "#  'pl_typ_lecby_10',\n",
    "#  'pl_kod_lecby_10',\n",
    " \n",
    " #### Hospitalizace #### \n",
    " 'pl_pocet_hp',\n",
    " 'pl_hp_los',\n",
    " 'pl_hp_od_stan',\n",
    " 'pl_hp_od_int',\n",
    " 'pl_hp_drg_prvni',\n",
    " 'pl_hp_drg_posledni',\n",
    "\n",
    " #### Vysetreni pri / po konci pl ####\n",
    " 'pl_ct',\n",
    " 'pl_petct',\n",
    " 'pl_mr',\n",
    " 'pl_scint',\n",
    " 'pl_spect',\n",
    " 'pl_rtg',\n",
    " 'pl_sono',\n",
    " 'pl_mamo',\n",
    " 'pl_jina',\n",
    "\n",
    " #### Disperzni vysetreni ####\n",
    " # nedavat, po skonceni pl\n",
    "#  'je_disp',\n",
    "#  'je_disp_prakt',\n",
    "#  'je_disp_onk',\n",
    "#  'je_disp_int',\n",
    "#  'je_disp_chir',\n",
    "#  'je_disp_gyn',\n",
    "\n",
    "#### Navstevy a body #### #nemuzem az do nl\n",
    " # 'amb_onk_navstevy', \n",
    "#  'amb_onk_body_rok1',\n",
    "#  'amb_onk_body_rok2',\n",
    "#  'amb_onk_body_rok3',\n",
    "#  'amb_onk_body_rok4',\n",
    "#  'amb_onk_body_rok5',\n",
    "#  'amb_int_navstevy',\n",
    "#  'amb_int_body_rok1',\n",
    "#  'amb_int_body_rok2',\n",
    "#  'amb_int_body_rok3',\n",
    "#  'amb_int_body_rok4',\n",
    "#  'amb_int_body_rok5',\n",
    "#  'amb_chir_navstevy',\n",
    "#  'amb_chir_body_rok1',\n",
    "#  'amb_chir_body_rok2',\n",
    "#  'amb_chir_body_rok3',\n",
    "#  'amb_chir_body_rok4',\n",
    "#  'amb_chir_body_rok5',\n",
    "#  'amb_gyn_navstevy',\n",
    "#  'amb_gyn_body_rok1',\n",
    "#  'amb_gyn_body_rok2',\n",
    "#  'amb_gyn_body_rok3',\n",
    "#  'amb_gyn_body_rok4',\n",
    "#  'amb_gyn_body_rok5',\n",
    "#  'amb_celkem_navstevy',\n",
    "#  'amb_celkem_body_rok1',\n",
    "#  'amb_celkem_body_rok2',\n",
    "#  'amb_celkem_body_rok3',\n",
    "#  'amb_celkem_body_rok4',\n",
    "#  'amb_celkem_body_rok5',\n",
    " \n",
    " #### Naslenda lecba ####\n",
    "#  'je_nl',\n",
    "#  'je_nl_koc',\n",
    "#  'je_nl_roc',\n",
    "#  'je_nl_jinde',\n",
    "#  'time_datum_dg_to_zahajeni_nl',\n",
    "#  'nl_delka',\n",
    "#  'je_nl_oper',\n",
    "#  'je_nl_radio',\n",
    "#  'je_nl_target',\n",
    "#  'je_nl_chemo',\n",
    "#  'je_nl_hormo',\n",
    "#  'je_nl_imuno',\n",
    "#  'nl_pocet_leceb',\n",
    "#  'nl_typ_lecby_1',\n",
    "#  'nl_kod_lecby_1',\n",
    "#  'nl_typ_lecby_2',\n",
    "#  'nl_kod_lecby_2',\n",
    "#  'nl_typ_lecby_3',\n",
    "#  'nl_kod_lecby_3',\n",
    "#  'nl_typ_lecby_4',\n",
    "#  'nl_kod_lecby_4',\n",
    "#  'nl_typ_lecby_5',\n",
    "#  'nl_kod_lecby_5',\n",
    "#  'nl_typ_lecby_6',\n",
    "#  'nl_kod_lecby_6',\n",
    "#  'nl_typ_lecby_7',\n",
    "#  'nl_kod_lecby_7',\n",
    "#  'nl_typ_lecby_8',\n",
    "#  'nl_kod_lecby_8',\n",
    "#  'nl_typ_lecby_9',\n",
    "#  'nl_kod_lecby_9',\n",
    "#  'nl_typ_lecby_10',\n",
    "#  'nl_kod_lecby_10',\n",
    "#  'nl_pocet_hp',\n",
    "#  'nl_hp_los',\n",
    "#  'nl_hp_od_stan',\n",
    "#  'nl_hp_od_int',\n",
    "#  'nl_hp_drg_prvni',\n",
    "#  'nl_hp_drg_posledni',\n",
    "\n",
    "#### Souhrn a smrti ####\n",
    "#  'souhrn_diagnoz',\n",
    "#  'nl_only_hormo',\n",
    "#  'nl_others',\n",
    "#\"relaps_5_years\",\n",
    "#\"dead_5_years_bc\",\n",
    "\"dead_relaps_5_years\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "465f48de-1909-442c-a6ab-7f6236885423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.read.table(\"rakathon_hackathon.rekurze.rekurze_death_initial_set_censured_oprava\") # all negative cases\n",
    "      .withColumn(\"stadium\", f.when(f.col(\"stadium\").isin(\"1\", \"2\", \"3\", \"4\"), f.col(\"stadium\")).otherwise(None))\n",
    "      .withColumn(\"stadium\", f.col(\"stadium\").cast(IntegerType()))\n",
    "      .select(*feature_cols_post_care)\n",
    "      .filter(f.col(\"je_pl\")==1)\n",
    "      .filter(f.col(\"dead_relaps_5_years\").isNotNull())\n",
    "      )\n",
    "\n",
    "#display(df.filter(f.col(\"je_pl\")==1).count())\n",
    "#display(df.filter(f.col(\"pl_delka\").isNotNull()).count())\n",
    "#display(df.count())\n",
    "display(df\n",
    "        #.filter(f.col(\"relaps_5_years\").isNotNull())\n",
    "        #.filter(f.col(\"dead_5_years_bc\").isNull())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5106c052-46e6-4a33-8186-97a04c4288a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_plot = df.select(\"stadium\", \"vekova_kategorie_10let_dg\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4470b94-888f-4050-a902-dc453b77c216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sns.violinplot(data=df_plot, x=\"vekova_kategorie_10let_dg\", y=\"stadium\", color=\"steelblue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "450826c6-a7b9-40cd-bcba-fc01378b723d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShkZikKCiMgdGFyZ2V0IDA6IDMxIDgzMQojIHRhcmdldCAxOiAxMyA5NjQKIyB0b3RhbDogNDkgOTM0CgojIHRubSBrbGFzaWZpa2FjZSBqc291IHbFoXVkZSAwLCDFvsOhZG7DoSBwcmVkaWt0aXZuw60gc8OtbGE=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewf753e86\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewf753e86\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewf753e86\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewf753e86) SELECT `dead_5_years_bc`,COUNT(*) `column_bc262a2816`,`dead_5_years_bc` FROM q GROUP BY `dead_5_years_bc`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewf753e86\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "dead_5_years_bc",
             "id": "column_42978ea516"
            },
            "x": {
             "column": "dead_5_years_bc",
             "id": "column_42978ea515"
            },
            "y": [
             {
              "column": "*",
              "id": "column_bc262a2816",
              "transform": "COUNT"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_bc262a2816": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "9113bb2b-0507-422d-9865-7b7baca4575d",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 3.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "dead_5_years_bc",
           "type": "column"
          },
          {
           "column": "dead_5_years_bc",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "dead_5_years_bc",
           "type": "column"
          },
          {
           "alias": "column_bc262a2816",
           "args": [
            {
             "column": "*",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          },
          {
           "column": "dead_5_years_bc",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShkZikKCiMgdGFyZ2V0IDA6IDMxIDgzMQojIHRhcmdldCAxOiAxMyA5NjQKIyB0b3RhbDogNDkgOTM0CgojIHRubSBrbGFzaWZpa2FjZSBqc291IHbFoXVkZSAwLCDFvsOhZG7DoSBwcmVkaWt0aXZuw60gc8OtbGE=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewdc59e5c\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewdc59e5c\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewdc59e5c\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewdc59e5c) SELECT `dead_relaps_5_years`,COUNT(*) `column_ecd1114297`,`dead_relaps_5_years` FROM q GROUP BY `dead_relaps_5_years`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewdc59e5c\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 2",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "dead_relaps_5_years",
             "id": "column_60e1e4c343"
            },
            "x": {
             "column": "dead_relaps_5_years",
             "id": "column_60e1e4c342"
            },
            "y": [
             {
              "column": "*",
              "id": "column_ecd1114297",
              "transform": "COUNT"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_ecd1114297": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1744499216694,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "table",
         2
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "033a52b2-312d-4066-973f-7e2fbe667c5f",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 4.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1744499215522,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "dead_relaps_5_years",
           "type": "column"
          },
          {
           "column": "dead_relaps_5_years",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "dead_relaps_5_years",
           "type": "column"
          },
          {
           "alias": "column_ecd1114297",
           "args": [
            {
             "column": "*",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          },
          {
           "column": "dead_relaps_5_years",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 1744499215469,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)\n",
    "\n",
    "# target 0: 31 831\n",
    "# target 1: 13 964\n",
    "# total: 49 934\n",
    "\n",
    "# tnm klasifikace jsou všude 0, žádná prediktivní síla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d690f7a-1e19-41d5-8d7f-cbcb2040b7b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Convert Spark DF to pandas\n",
    "df_pd = df.toPandas()\n",
    "\n",
    "# 2. Define X, y\n",
    "target_col = \"dead_relaps_5_years\"\n",
    "X = df_pd.drop(columns=[target_col])\n",
    "y = df_pd[target_col]\n",
    "\n",
    "# 3. Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "\n",
    "# 4. Define preprocessor (OneHotEncoder only for categorical)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        #(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "        (\"num\", \"passthrough\", numerical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 5. Define pipeline with preprocessor + model\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", XGBClassifier(\n",
    "        use_label_encoder=True,\n",
    "        eval_metric=\"auc\",\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.01,\n",
    "        subsample=0.8\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 6. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "# 7. MLflow tracking\n",
    "with mlflow.start_run(run_name=\"rackathon_xgboost_pipeline_relaps\"):\n",
    "    # Log parameters manually\n",
    "    mlflow.log_params({\n",
    "        \"n_estimators\": 1000,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"subsample\": 0.8,\n",
    "        \"eval_metric\": \"auc\"\n",
    "    })\n",
    "\n",
    "    # Fit pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"recall\": recall\n",
    "    })\n",
    "\n",
    "    # Log pipeline model (includes preprocessing)\n",
    "    mlflow.sklearn.log_model(pipeline, artifact_path=\"xgb_pipeline_model\")\n",
    "\n",
    "    print(f\"✅ Accuracy: {accuracy:.4f} | F1: {f1:.4f} | Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae2d3d2b-2508-468e-8583-9c7534f73ab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90f52fad-a33a-4000-b211-b814c42a6ca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_pred_proba_b = [x[1] for x in y_pred_proba]\n",
    "df_test = pd.DataFrame({\"y_test_gt\":y_test, \"y_test_pred_proba\":y_pred_proba_b})\n",
    "\n",
    "sns.displot(\n",
    "    data=df_test,\n",
    "    x=\"y_test_pred_proba\",\n",
    "    hue=\"y_test_gt\",\n",
    "    #kind=\"kde\",             # or \"hist\"\n",
    "    common_norm=False,      # ← normalize within each class\n",
    "    stat=\"density\",         # ensures comparable y-axis\n",
    "    height=7,\n",
    "    aspect=2\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a37b845a-11ea-445f-abe1-9526f1101bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3250c736-ea81-4750-a5d8-4086ddb4db38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict probabilities for class 1\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall pairs and AUPRC\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "auprc = average_precision_score(y_test, y_proba)\n",
    "\n",
    "# Plot PR curve\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(recall, precision, label=f\"AUPRC = {auprc:.4f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73824889-3a3b-4a36-83ef-c4339f89a1fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Extract model and preprocessor from pipeline\n",
    "model = pipeline.named_steps[\"classifier\"]\n",
    "preprocessor = pipeline.named_steps[\"preprocessor\"]\n",
    "\n",
    "# 2. Get feature names after one-hot + passthrough\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# 3. Get XGBoost feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# 4. Create DataFrame with importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importances\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# 6. Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=feature_importance_df.head(20),\n",
    "    x=\"importance\",\n",
    "    y=\"feature\"\n",
    ")\n",
    "plt.title(\"Top 20 Feature Importances (XGBoost)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.grid(True, axis='x', linestyle='--', linewidth=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca7ff7d4-d208-4b1c-a1ff-5ab7035c7fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## shapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddec8da1-d148-4b06-a2f1-6a000a64dc75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Extract model and preprocessor from pipeline\n",
    "model = pipeline.named_steps[\"classifier\"]\n",
    "preprocessor = pipeline.named_steps[\"preprocessor\"]\n",
    "\n",
    "# 2. Transform X_test using the fitted preprocessor\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# 3. Get feature names from preprocessor\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# 4. Convert transformed data to DataFrame with correct feature names\n",
    "X_test_transformed_df = pd.DataFrame(\n",
    "    X_test_transformed.toarray() if hasattr(X_test_transformed, \"toarray\") else X_test_transformed,\n",
    "    columns=feature_names\n",
    ")\n",
    "\n",
    "# 5. Create SHAP explainer for the XGBoost model\n",
    "explainer = shap.Explainer(model)\n",
    "\n",
    "# 6. Compute SHAP values\n",
    "shap_values = explainer(X_test_transformed_df)\n",
    "\n",
    "# 7. Plot SHAP summary with feature names\n",
    "shap.initjs()\n",
    "shap.summary_plot(shap_values, X_test_transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc84912-08d4-4568-bd6e-5f25932c0797",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "claude"
    }
   },
   "outputs": [],
   "source": [
    "# Index of the highest probability\n",
    "max_index = np.argmax(y_proba)\n",
    "min_index = np.argmin(y_proba)\n",
    "\n",
    "# Value at that index\n",
    "max_proba = y_proba[max_index]\n",
    "min_proba = y_proba[min_index]\n",
    "\n",
    "print(f\"🔥 Highest predicted probability is {max_proba:.4f} at index {max_index}\")\n",
    "print(f\"🔥 Highest predicted probability is {min_proba:.4f} at index {min_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ecf0ef-a54b-41a1-8cbd-f80882306002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: get preprocessed test data\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Step 2: convert to DataFrame for explainability\n",
    "X_test_transformed_df = pd.DataFrame(\n",
    "    X_test_transformed.toarray() if hasattr(X_test_transformed, \"toarray\") else X_test_transformed,\n",
    "    columns=feature_names\n",
    ")\n",
    "\n",
    "def explain_prediction(i, model, explainer, shap_values, X_test_transformed_df, original_X_test):\n",
    "    print(f\"\\n🔍 Explanation for Row #{i}\")\n",
    "    proba = pipeline.predict_proba(original_X_test.iloc[[i]])[0, 1]\n",
    "    print(f\"🔮 Predicted Probability of relaps in 5 years: {proba:.4f}\\n\")\n",
    "    \n",
    "    shap.initjs()\n",
    "    return shap.force_plot(\n",
    "        explainer.expected_value,\n",
    "        shap_values[i].values,\n",
    "        X_test_transformed_df.iloc[i],\n",
    "        matplotlib=True\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "explain_prediction(14, model, explainer, shap_values, X_test_transformed_df, X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34b7f8a9-6906-4afa-803e-fba0e58bc0a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## mappin to patient id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d9d98b5-70f1-4376-93fc-32ebce642ef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 0 is nice\n",
    "# 1. Select the row to match\n",
    "display(X_test.iloc[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f182fcba-1967-4ce7-be35-e11fa36e03e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 213730 - id\n",
    "row_df = (spark.read.table(\"rakathon_hackathon.rekurze.rekurze_death_initial_set_censured\")\n",
    "          .select(*feature_cols_post_care)\n",
    "        .filter(f.col(\"id\")==213730))\n",
    "\n",
    "# 2. Convert to pandas and transpose\n",
    "row_pd = row_df.toPandas().T  # Transpose: features as index\n",
    "\n",
    "# 3. Rename column and reset index for nice display\n",
    "row_pd.columns = [\"Value\"]\n",
    "row_pd = row_pd.reset_index().rename(columns={\"index\": \"Feature\"})\n",
    "\n",
    "# 4. Cast values to string to avoid Arrow issues with display()\n",
    "row_pd[\"Value\"] = row_pd[\"Value\"].astype(str)\n",
    "\n",
    "# 5. Display\n",
    "display(row_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbea4a9b-9693-47ae-abad-b97ed1e3f9a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Read the original row by ID\n",
    "original_df = (\n",
    "    spark.read.table(\"rakathon_hackathon.rekurze.rekurze_death_initial_set_censured\")\n",
    "    .filter(f.col(\"id\") == 213730)\n",
    ")\n",
    "\n",
    "# 2. Modify the stadium column from 1 to 3\n",
    "modified_df = original_df.withColumn(\"je_pl_hormo\", f.lit(0)) # je_pl_koc\n",
    "\n",
    "# 3. Union original + modified row (optional)\n",
    "inference_df = original_df.unionByName(modified_df).select(*feature_cols_post_care)\n",
    "\n",
    "# 4. Show both rows (for verification)\n",
    "display(inference_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfecd389-f2e5-48ef-8518-f812d9b29e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's assume your modified Spark DataFrame is called `result_df`\n",
    "pandas_input = inference_df.toPandas()\n",
    "X_new = pandas_input.drop(columns=[target_col,\"id\"], errors=\"ignore\")  # or add 'relaps_5_years' if it's present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2585466-c06d-4144-9d93-679143df2fa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict class\n",
    "pred_class = pipeline.predict(X_new)\n",
    "\n",
    "# Predict probability (positive class, e.g. relaps in 5 years)\n",
    "pred_proba = pipeline.predict_proba(X_new)[:, 1]\n",
    "\n",
    "\n",
    "display(pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c6afec-7f2f-492d-b6ff-04991dab200c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: get preprocessed test data\n",
    "X_test_transformed = preprocessor.transform(X_new)\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Step 2: convert to DataFrame for explainability\n",
    "X_test_transformed_df = pd.DataFrame(\n",
    "    X_test_transformed.toarray() if hasattr(X_test_transformed, \"toarray\") else X_test_transformed,\n",
    "    columns=feature_names\n",
    ")\n",
    "\n",
    "def explain_prediction(i, model, explainer, shap_values, X_test_transformed_df, original_X_test):\n",
    "    print(f\"\\n🔍 Explanation for Row #{i}\")\n",
    "    proba = pipeline.predict_proba(original_X_test.iloc[[i]])[0, 1]\n",
    "    print(f\"🔮 Predicted Probability of relaps in 5 years: {proba:.4f}\\n\")\n",
    "    \n",
    "    shap.initjs()\n",
    "    return shap.force_plot(\n",
    "        explainer.expected_value,\n",
    "        shap_values[i].values,\n",
    "        X_test_transformed_df.iloc[i],\n",
    "        matplotlib=True\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "explain_prediction(0, model, explainer, shap_values, X_test_transformed_df, X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1879d35b-a620-4cfe-9946-d58865d60ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "explain_prediction(1, model, explainer, shap_values, X_test_transformed_df, X_new) # Tomuhle přidat hormono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d40fedbc-94a7-4633-b8c0-df45ee7a63a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae60545-69da-4993-af71-96c98a9dd2aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "# 1. Get predicted probabilities for positive class (relaps=1)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2. Compute calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_proba, n_bins=10, strategy=\"quantile\")\n",
    "\n",
    "# 3. Plot calibration curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(prob_pred, prob_true, marker='o', label=\"XGBoost Pipeline\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Perfect Calibration\")\n",
    "plt.xlabel(\"Mean Predicted Probability\")\n",
    "plt.ylabel(\"True Fraction of Positives\")\n",
    "plt.title(\"Calibration Curve (Reliability Diagram)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Optional: plot histogram of predicted probabilities\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.hist(y_proba, bins=20, edgecolor='k')\n",
    "plt.title(\"Histogram of Predicted Probabilities\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "modelling_death_lb_relaps_3.0",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
